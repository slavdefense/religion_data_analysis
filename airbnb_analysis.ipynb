{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Cleaning & Preparation\n",
    "## Using Airbnb Data\n",
    "\n",
    "<img src='airbnb_photo.JPG'>\n",
    "<i>Image by <a href=\"https://pixabay.com/users/instagramfotografin-5746148/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=3399753\">InstagramFOTOGRAFIN</a> from <a href=\"https://pixabay.com/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=3399753\">Pixabay</a></i>\n",
    "\n",
    "It is often said that an analyst spends 80% or more of their time on data preparation tasks such as cleaning, transforming and rearranging data. In this module, we will discuss ways to handle missing data, duplicate data, string manipulation and other types of transformations.\n",
    "\n",
    "When we first explore a topic, we will use a simple example to explain the process. Then you will practice by implementing a similar transformation using a real world dataset in a more complex way.\n",
    "\n",
    "For this module, we will be working with data from [Airbnb](https://www.airbnb.com/), one of the top travel websites where hosts can list their properties or rooms for vacation rental. The ultimate goal of working with this data is to see if we can predict a property's price based on its specific attributes. We will specifically be using data from Vienna, Austria.\n",
    "\n",
    "The Airbnb city data comes from [Inside Airbnb](http://insideairbnb.com/get-the-data.html), an investigatory website that focuses on highlighting illegal renting through Airbnb and how Airbnb affects property values in an area. The data is sourced directly from the Airbnb website and is updated monthly. \n",
    "\n",
    "The data is ‘messy’ with long text fields, large amounts of missing data and many features that might not have any correlation with prices. An example of the ‘messiness’ of the data can be found in the ‘amenities’ feature. Each property has a list of amenities that it provides, some standard from the Airbnb website and some that are entered directly by the property owners. This data will need to be pulled apart and cleaned, with new features added based on these amenities.\n",
    "\n",
    "**Note 1:** In order to better show examples of data transformations, some of the data has been adjusted for teaching purposes. If you want to do any further data analysis, please download the actual data from the link above.\n",
    "\n",
    "**Note 2:** We are only using Pandas and Numpy for these data transformations. In future classes, you will learn about additional, sometimes easier, options such as using Scikit-learn for some of these transformations. But even these options use Pandas and Numpy as their base.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Import Data](#import)\n",
    "2. [Initial Data Exploration](#exploration)\n",
    "3. [Missing Data](#missing)\n",
    "[<ul>Filtering Missing Data</ul>](#filter)\n",
    "[<ul>Dropping Missing Data</ul>](#drop)\n",
    "[<ul>Filling Missing Data</ul>](#filling)\n",
    "4. [Data Transformation](#transformation)\n",
    "[<ul>Removing Duplicates</ul>](#remove_dupes)\n",
    "[<ul>Transforming Data Using a Function or Mapping</ul>](#map)\n",
    "[<ul>Replacing Values</ul>](#replace)\n",
    "[<ul>Binning</ul>](#bin)\n",
    "[<ul>Detecting & Filtering Outliers</ul>](#outlier)\n",
    "[<ul>Dummy Variables</ul>](#dummy)\n",
    "5. [String Manipulation](#string)\n",
    "[<ul>String Object Methods</ul>](#string_object)\n",
    "[<ul>Regular Expressions</ul>](#regex)\n",
    "6. [Extra Practice](#import)\n",
    "7. [Conclusion](#conclusion)\n",
    "\n",
    "## Import Data<a name=\"import\"></a>\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# set display columns to max so that we can view all columns\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# import the Vienna dataset\n",
    "vienna_raw = pd.read_csv('listings_vienna.csv', index_col='id')\n",
    "\n",
    "# saving raw data as new dataframe for exploration\n",
    "vienna = vienna_raw.copy()\n",
    "\n",
    "## Initial Data Exploration<a name=\"exploration\"></a>\n",
    "\n",
    "# view first few rows\n",
    "vienna.head()\n",
    "\n",
    "# view number of rows/columns\n",
    "vienna.shape\n",
    "\n",
    "vienna.info()\n",
    "\n",
    "There are three different datatypes represented in the data. Notice that the `price` column, which will be our label (ie what we are trying to predict), is represented as a string instead of a number.  We will need to change this before running any type of machine learning algorithm.  Also, there are several features that have missing values. We'll take care of these as we go along.\n",
    "\n",
    "We will analyze more of the features later, but let's first look at a few of the features in further detail: `neighborhoods in Vienna`, `property types`, `room types`, and `amenities`.\n",
    "\n",
    "# view value counts for neighborhood_cleansed\n",
    "# k =vienna.groupby(['neighbourhood_cleansed']).size()\n",
    "# k\n",
    "vienna['neighbourhood_cleansed'].value_counts()\n",
    "\n",
    "# view value counts for property_type\n",
    "vienna['property_type'].value_counts()\n",
    "\n",
    "There are a lot of different values for this feature. We could combine some of these (example: cabin and chalet should be similar to a house) to make our list more manageable. But first, let's also look at `room_type`.\n",
    "\n",
    "# view value counts for room_type\n",
    "vienna['room_type'].value_counts()\n",
    "\n",
    "This seems to be similar data as the `property_type`. I'll leave both in for now and you would want to check for multi-linear correlation on your own if you were actually working on this as a project.\n",
    "\n",
    "# view value counts for amenities\n",
    "vienna['amenities']\n",
    "\n",
    "The `amenities` feature is a list of available amenities for each property. This will be difficult to work with but some of these listed amenities might have an influence on the price. We will work on separating these list items out later.\n",
    "\n",
    "After a manual review of each feature, there are several features that we can immediately remove from the dataset. First, we can drop features that are not related to price.\n",
    "\n",
    "drop_features = ['listing_url','scrape_id','host_url','host_name','host_location',\n",
    "                'host_neighbourhood','neighbourhood_group_cleansed','calendar_updated','license',\n",
    "                'host_thumbnail_url','host_picture_url','host_verifications','host_has_profile_pic',\n",
    "                'has_availability','host_total_listings_count','neighbourhood',\n",
    "                'minimum_minimum_nights','maximum_minimum_nights','minimum_maximum_nights','maximum_maximum_nights',\n",
    "                'minimum_nights_avg_ntm','maximum_nights_avg_ntm','availability_30','availability_60',\n",
    "                'availability_365','calendar_last_scraped','number_of_reviews_ltm','number_of_reviews_l30d',\n",
    "                'calculated_host_listings_count_entire_homes','calculated_host_listings_count_private_rooms',\n",
    "                'calculated_host_listings_count_shared_rooms','last_scraped','neighborhood_overview',\n",
    "                 'host_about','picture_url','host_identity_verified', 'minimum_nights', 'maximum_nights', \n",
    "                 'availability_90', 'instant_bookable','bathrooms','host_total_listings_count']\n",
    "\n",
    "vienna = vienna.drop(drop_features, axis=1)\n",
    "vienna.head()\n",
    "\n",
    "vienna.shape\n",
    "\n",
    "**Important:** In a real world project, we would stop here and split the data into what is called a training set and a test set. The training set is used to explore and train the data, and the test set is used at the very end to determine how new data generalizes with your model. This will be covered in much more detail in the machine learning classes.\n",
    "\n",
    "## Missing Data<a name=\"missing\"></a>\n",
    "Missing data is very common in real world datasets and Pandas is a powerful tool in working with this missing data.\n",
    "\n",
    "### Filtering Missing Data<a name=\"filter\"></a>\n",
    "\n",
    "Let's now look at some of the missing data in the dataset. First, we need to understand what data is missing (we should attempt to either find the missing data or impute it based on other data) and what data just doesn't exist (where that fact that it is missing might provide additional information -- example: no reviews for a property).\n",
    "\n",
    "# creates True/False mask\n",
    "vienna['beds'].isnull()\n",
    "\n",
    "vienna['beds'].isnull().sum() # missing from data\n",
    "\n",
    "This could be an example of purely missing data. Every property probably should have at least one bed listed, even if it is a non-standard bed type.\n",
    "\n",
    "vienna['first_review'].isnull().sum() # doesn't exist\n",
    "\n",
    "This is an example of data that probably just doesn't exist. These properties might not have any reviews from guests. And the fact that they don't have any reviews might be important to the properties price.\n",
    "\n",
    "### Dropping missing values<a name=\"drop\"></a>\n",
    "One way to handle missing data is to drop the values completely. This could be appropriate based on your data but remember that you might lose some other information by dropping the data.\n",
    "\n",
    "#setup simple DataFrame\n",
    "data = pd.DataFrame([[2.,4.,6.],[1.,np.nan,np.nan],\n",
    "                     [np.nan,np.nan,np.nan],[np.nan,10.,12.]])\n",
    "data\n",
    "\n",
    "# using drop_na to drop rows with missing values\n",
    "data.dropna()\n",
    "\n",
    "# dropping rows where there are missing values only for column '1'\n",
    "data.dropna(subset=[1])\n",
    "\n",
    "# using drop_na to drop columns with missing values\n",
    "data.dropna(axis=1)\n",
    "\n",
    "# using drop_na to drop rows that have all missing values\n",
    "data.dropna(how='all')\n",
    "\n",
    "# using drop_na to drop rows that don't have at least 2 values\n",
    "data.dropna(thresh=2)\n",
    "\n",
    "#### Student Practice\n",
    "Try to perform the following tasks on the `vienna` dataset. Then check your answers as I walk through the solutions. Unless the exercise asks you to create a new DataFrame, you can assume that you are to alter the original `vienna` data.\n",
    "\n",
    "# run the following code\n",
    "vienna.shape\n",
    "\n",
    "**Exercise:** Create a new DataFrame called `vienna_dropped_rows` to drop all rows from the original `vienna` dataset with missing values. \n",
    "\n",
    "### ENTER CODE HERE ###\n",
    "vienna_dropped_rows = pd.DataFrame(vienna)\n",
    "vienna_dropped_rows.dropna()\n",
    "\n",
    "**Exercise:** Create a new DataFrame called `vienna_dropped_rows_subset` where rows with missing values in the `bedrooms` column are dropped.\n",
    "\n",
    "### ENTER CODE HERE ###\n",
    "vienna_dropped_rows_subset = pd.DataFrame(vienna)\n",
    "# vienna_dropped_rows_subset.loc[:,'bedrooms'].dropna()\n",
    "vienna_dropped_rows_subset.dropna(subset=['bedrooms'])\n",
    "\n",
    "**Exercise:** Create a new DataFrame called `vienna_dropped_cols` where columns with missing values are dropped.\n",
    "\n",
    "### ENTER CODE HERE ###\n",
    "veinna_dropped_cols = pd.DataFrame(vienna)\n",
    "veinna_dropped_cols.dropna(axis=1)\n",
    "\n",
    "**Exercise:** Drop rows from the `vienna` dataset that have only missing values. \n",
    "\n",
    "### ENTER CODE HERE ###\n",
    "vienna.dropna(how='all')\n",
    "\n",
    "**Exercise:** Drop rows from the `vienna` dataset that don't have at least 10 values.\n",
    "\n",
    "### ENTER CODE HERE ###\n",
    "vienna.dropna(thresh=10)\n",
    "\n",
    "### Filling Missing Data<a name=\"filling\"></a>\n",
    "\n",
    "Instead of simply discarding data and potentially losing information from other data that goes along with it, sometimes it's better to fill the missing data. This can be done in a number of different ways.\n",
    "\n",
    "# setup example DataFrame\n",
    "data = pd.DataFrame(np.random.randn(7, 3))\n",
    "data.iloc[:4, 1] = np.nan\n",
    "data.iloc[:2, 2] = np.nan\n",
    "data\n",
    "\n",
    "# using fillna with a constant \n",
    "data.fillna(0)\n",
    "\n",
    "# using fillna with a dictionary for potential different values\n",
    "data.fillna({1: 1.5, 2:-1.5})\n",
    "\n",
    "# same interpolation methods available for reindexing can be used with fillna\n",
    "df = pd.DataFrame(np.random.randn(6, 3))\n",
    "df.iloc[2:, 1] = np.nan\n",
    "df.iloc[4:, 2] = np.nan\n",
    "df\n",
    "\n",
    "# using 'ffill'\n",
    "df.fillna(method='ffill')\n",
    "\n",
    "# using 'ffill' with limit\n",
    "df.fillna(method='ffill', limit=2)\n",
    "\n",
    "# using the mean of each column\n",
    "df.fillna(df.mean())\n",
    "\n",
    "#### Student Practice\n",
    "Try to perform the following tasks on the `vienna` dataset. Then check your answers as I walk through the solutions. Unless the exercise asks you to create a new DataFrame, you can assume that you are to alter the original `vienna` data.\n",
    "\n",
    "# look at missing data for bathrooms\n",
    "vienna['bathrooms_text'].isnull().sum()\n",
    "\n",
    "**Exercise:** Fill missing values from `bathrooms_text` with the number `1`.\n",
    "\n",
    "### ENTER CODE HERE ###\n",
    "vienna['bathrooms_text'].fillna(1, inplace=True)\n",
    "vienna['bathrooms_text'].value_counts()\n",
    "\n",
    "**Exercise:** Fill missing values as follows:\n",
    "- bedrooms: 1\n",
    "- host_listings_count: 1\n",
    "- host_is_superhost: 'f'\n",
    "\n",
    "### ENTER CODE HERE ###\n",
    "vienna[vienna['bedrooms'].isna() | vienna['host_listings_count'].isna() | vienna['host_is_superhost'].isna()][['bedrooms','host_listings_count','host_is_superhost']]\n",
    "\n",
    "vienna.fillna({'bedrooms':1, 'host_listings_count':1, 'host_is_superhost':'f'}, inplace=True)\n",
    "\n",
    "\n",
    "**Exercise:** Fill the missing values in the `beds` column with the median.\n",
    "\n",
    "### ENTER CODE HERE ###\n",
    "# vienna['beds'].fillna(value=vienna.median)\n",
    "vienna['beds'].fillna(vienna['beds'].median(), inplace=True)\n",
    "vienna['beds'].isnull().sum()\n",
    "\n",
    "**Note:** There are more efficient ways to handle missing values by using pipelines with Scikit-learn, which you will study more in the machine learning classes. These pipelines will allow you to clean future data more easily so that you won't have to do it manually each time.\n",
    "\n",
    "## Data Transformation<a name=\"transformation\"></a>\n",
    "### Removing Duplicates<a name=\"remove_dupes\"></a>\n",
    "You may also find yourself spending a lot of time identifying and handling duplicate data. Sometimes it will be easy to identify the duplicates (all or most of the values are the same), while other times duplicates will be much harder to identify.\n",
    "\n",
    "# create basic DataFrame\n",
    "data = pd.DataFrame({'Student': ['Stephanie','Nadia','Lukas','Sally','Nadia','Nadia'],\n",
    "                    'Grade': [100,95,100,80,100,100]})\n",
    "data\n",
    "\n",
    "# returns boolean Series: True represents whether row has been observed before\n",
    "data.duplicated()\n",
    "\n",
    "# returns only non-duplicated rows\n",
    "data.drop_duplicates()\n",
    "\n",
    "# keep the last row instead of the first\n",
    "data.drop_duplicates(keep='last')\n",
    "\n",
    "# default considers all values\n",
    "# can specify subset \n",
    "data.drop_duplicates(['Student'])\n",
    "\n",
    "#### Student Practice\n",
    "Try to perform the following tasks on the `vienna` dataset. Then check your answers as I walk through the solutions. Unless the exercise asks you to create a new DataFrame, you can assume that you are to alter the original `vienna` data.\n",
    "\n",
    "**Exercise:** How many exact duplicates are in the `vienna` data? Drop any exact duplicates from the data.\n",
    "\n",
    "### ENTER CODE HERE ###\n",
    "vienna.duplicated()\n",
    "vienna.drop_duplicates(inplace=True)\n",
    "vienna.duplicated().sum()\n",
    "\n",
    "**Exercise:** How many properties have the exact same `host_id`, `name`, and `description`and are in the same neighborhood as a previous property?\n",
    "\n",
    "### ENTER CODE HERE ###\n",
    "# vienna['description'].duplicated().sum() + vienna['name'].duplicated().sum()+ vienna['description'].duplicated().sum()\n",
    "vienna.duplicated(['host_id', 'name','description', 'neighbourhood_cleansed']).sum()\n",
    "\n",
    "**Exercise**: These properties may be duplicates. Let's say after researching further that we have decided to remove these duplicated rows. Remove these rows from the `vienna` data.\n",
    "\n",
    "### ENTER CODE HERE ###\n",
    "vienna.drop_duplicates(['host_id', 'name','description', 'neighbourhood_cleansed'],inplace=True)\n",
    "\n",
    "\n",
    "### Transforming Data Using a Function or Mapping<a name=\"map\"></a>\n",
    "Performing transformations based on values in an array.\n",
    "\n",
    "\n",
    "# create a sample DataFrame\n",
    "data = pd.DataFrame({\n",
    "    'city': ['atlanta','Baltimore','boston','Buffalo','charlotte'],\n",
    "    'state': ['GA','Maryland','Massachusetts','New York','NC']\n",
    "})\n",
    "\n",
    "data\n",
    "\n",
    "First, notice that some of these cities are capitlized and some are not. We need to keep that in mind when we go to map the values.\n",
    "\n",
    "Suppose you wanted to add a column indicating the NFL mascot for the respective city. We can map the city to the mascot name as follows:\n",
    "\n",
    "# create a dictionary mapping\n",
    "city_to_mascot = {\n",
    "    'atlanta':'Falcons',\n",
    "    'baltimore':'Ravens',\n",
    "    'boston':'Patriots',\n",
    "    'buffalo':'Bills',\n",
    "    'charlotte':'Panthers'\n",
    "}\n",
    "\n",
    "# use `str.lower()` to convert values before mapping\n",
    "lowercased_city = data['city'].str.lower()\n",
    "lowercased_city\n",
    "\n",
    "# `map()` accepts a function or dictionary-like object\n",
    "data['mascot'] = lowercased_city.map(city_to_mascot)\n",
    "data\n",
    "\n",
    "# also can pass a function\n",
    "data['city'].map(lambda x: city_to_mascot[x.lower()])\n",
    "\n",
    "Sometimes you might only want to map some values and leave others as they appear in the data. You can combine `map()` and `fillna()` for this purpose.\n",
    "\n",
    "How could we use a mapping to abbreviate the states that don't yet have abbreviations?\n",
    "\n",
    "state_abbr = {\n",
    "    'Maryland':'MD',\n",
    "    'Massachusetts':'MA',\n",
    "    'New York':'NY'\n",
    "}\n",
    "\n",
    "# make a copy of data to use with the following example\n",
    "data2 = data.copy()\n",
    "\n",
    "# this won't work\n",
    "data2['state']= data2['state'].map(state_abbr)\n",
    "data2\n",
    "\n",
    "data['state'] = data['state'].map(state_abbr).fillna(data['state'])\n",
    "data\n",
    "\n",
    "#### Student Practice\n",
    "Try to perform the following tasks on the `vienna` dataset. Then check your answers as I walk through the solutions. Unless the exercise asks you to create a new DataFrame, you can assume that you are to alter the original `vienna` data.\n",
    "\n",
    "**Exercise:** Change the column name from the British spelling of `neighbourhood_cleansed` to the American spelling of `neighborhood_cleansed`.\n",
    "\n",
    "### ENTER CODE HERE ###\n",
    "vienna.rename(columns={'neighbourhood_cleansed':'neighborhood_cleansed'}, inplace=True)\n",
    "vienna\n",
    "\n",
    "**Exercise:** Check the value counts of this column. Notice that the various foreign language characters did not get encoded correctly.\n",
    "\n",
    "### ENTER CODE HERE ###\n",
    "vienna['neighborhood_cleansed'].value_counts()\n",
    "\n",
    "**Exercise:** Use a mapping to update the following neighborhood names. Check your value counts again to ensure that the names were updated. \n",
    "- Landstrasse\n",
    "- Rudolfsheim-Funfhaus\n",
    "- Wahring\n",
    "- Dobling\n",
    "\n",
    "### ENTER CODE HERE ###\n",
    "updateNames = {'Landstra§e':'Landstrasse', 'Rudolfsheim-Fnfhaus':'Rudolfsheim-Funfhaus','Whring':'Wahring','Dbling':'Dobling' }\n",
    "vienna['neighborhood_cleansed']=  vienna['neighborhood_cleansed'].map(updateNames ).fillna(vienna['neighborhood_cleansed'])\n",
    "vienna['neighborhood_cleansed'].value_counts()\n",
    "\n",
    "**Exercise:** Using Seaborn, plot a scatterplot using the `longitude` and `latitude` of the `vienna` DataFrame using the `neighborhood_cleansed` column as the hue.\n",
    "\n",
    "### ENTER CODE HERE ###\n",
    "plt.figure(figsize=(12,8))\n",
    "g = sns.scatterplot(x= vienna['longitude'], y=vienna['latitude'], hue=vienna['neighborhood_cleansed'])\n",
    "plt.show()\n",
    "# for i in vienna.columns:\n",
    "#     print(i)\n",
    "\n",
    "### Replacing Values<a name=\"replace\"></a>\n",
    "As we saw previously, `map` can be used to modify a subset of values but `replace` can sometimes be simpler and more flexible. However, using `map` is sometimes more efficient for larger datasets.\n",
    "\n",
    "data\n",
    "data['mascot']= ['Falcons', 'Ravens', 'Patriots', 'Bills', 'Panthers']\n",
    "data\n",
    "\n",
    "# using replace to replace a single value\n",
    "data.replace('Panthers', np.nan, inplace=True)\n",
    "data\n",
    "\n",
    "# using replace to replace multiple values\n",
    "data.replace(['Falcons', 'Ravens'], np.nan, inplace=True)\n",
    "data\n",
    "\n",
    "# using replace to replace different values using lists\n",
    "data.replace(['Patriots','Bills'],[np.nan, 'Wings'])\n",
    "\n",
    "# using replace to replace different values using a dictionary\n",
    "data.replace({'Patriots':np.nan, 'Bills':'Wings'}, inplace=True)\n",
    "data\n",
    "\n",
    "#### Student Practice\n",
    "Try to perform the following tasks on the `vienna` dataset. Then check your answers as I walk through the solutions. Unless the exercise asks you to create a new DataFrame, you can assume that you are to alter the original `vienna` data.\n",
    "\n",
    "**Exercise:** Check the value counts for the property types.\n",
    "\n",
    "### ENTER CODE HERE ###\n",
    "vienna['property_type'].value_counts()\n",
    "\n",
    "**Exercise:** We want to combine some of these categories so that we eventually only have four categories: 'Apartment', 'House', 'Room', and 'Other'. For example, 'Entire house' and 'Entire cottage' can probably just be listed as 'House' to simplify the data.\n",
    "\n",
    "First, combine the property types that you think could be simply represented by 'House' and 'Apartment' (Note: answers will vary based on how you break down the categories)\n",
    "\n",
    "### ENTER CODE HERE ###Tiny house\n",
    "house = ['Castle','Entire place' 'Entire hostel',\n",
    "         'Entire place',                     \n",
    "'Entire bungalow',                  \n",
    "'Dome house',                       \n",
    "'Entire hostel',    \n",
    "         \n",
    "         'Entire chalet', 'Casa particular', 'Lighthouse','Tiny house', 'Entire cottage','Entire villa', 'Entire guesthouse',\n",
    "'Entire house']\n",
    "apartment = ['Entire cabin','Entire townhouse','Entire serviced apartment','Entire guest suite', 'Camper/RV', 'Room in bed and breakfast', 'Entire loft', 'Entire condominium', 'Entire apartment']\n",
    "\n",
    "vienna['property_type'].replace(house, 'house', inplace=True)\n",
    "vienna['property_type'].replace(apartment, 'apartment', inplace=True)\n",
    "vienna['property_type'].value_counts()\n",
    "\n",
    "\n",
    "**Exercise:** Perform a similar task of combining property types that might be simply considered as a 'Room'. \n",
    "\n",
    "*Bonus:* You could perform this with `replace()` but it would take a long dictionary of values. Can you perform this using the Pandas [str.contains()](https://pandas.pydata.org/docs/reference/api/pandas.Series.str.contains.html) method in one line of code?\n",
    "\n",
    "### ENTER CODE HERE ###\n",
    "\n",
    "# room = ['Private room in apartment', 'Private room in house', 'Room in boutique hotel',\n",
    "#         'Private room in condominium', 'Shared room in apartment',\n",
    "#         'Private room in bed and breakfast', 'Room in hotel',\n",
    "#         'Private room in loft', 'Room in aparthotel','Private room in townhouse',\n",
    "#         'Private room in hostel',\n",
    "#         'Private room in guesthouse',\n",
    "#         'Shared room in hostel',\n",
    "#         'Room in bed and breakfast',\n",
    "#         'Private room in serviced apartment',\n",
    "#         'Private room in guest suite', 'Camper/RV', 'Shared room in condominium',\n",
    "#          'Shared room in loft', 'Private room in villa', 'Private room', 'Private room in earth house',\n",
    "#         'Private room in castle', 'Shared room in house',\n",
    "#         'Shared room in serviced apartment', 'Shared room in bed and breakfast',\n",
    "#         'Private room in farm stay', 'Private room in nature lodge',\n",
    "#         'Shared room in tiny house', 'Shared room in hotel', 'Private room in camper/rv',\n",
    "#         'Private room in cave', 'Private room in bus',\n",
    "#         'Private room in treehouse', 'Shared room in tent']  \n",
    "# vienna['property_type'].replace(room, 'room', inplace=True)\n",
    "# vienna['property_type'].value_counts()\n",
    "\n",
    "\n",
    "\n",
    "**Exercise:** Change all other values not listed as 'House', 'Apartment', or 'Room' to 'Other'. \n",
    "\n",
    "*Bonus:* Again, you could perform this with `replace()`, but can you think of a way to do it in one line of code? Hint: You want to select everything where the property type is not 'House', 'Apartment', or 'Room' and code those as 'Other'.   \n",
    "\n",
    "### ENTER CODE HERE ###\n",
    "vienna.loc[~vienna.property_type.isin(['room', 'house','apartment']), 'property_type']= 'Others'\n",
    "\n",
    "vienna['property_type'].value_counts()\n",
    "\n",
    "**Exercise:** Check the property type value counts one last time to verify that you now only have four categories.\n",
    "\n",
    "### ENTER CODE HERE ###\n",
    "\n",
    "### Binning<a name=\"bin\"></a>\n",
    "\n",
    "Continuous data is often separated into 'bins' for analysis.\n",
    "\n",
    "# setup list of birth years\n",
    "year = [1955, 1964, 1975, 1985, 1960, 2001, 1978]\n",
    "\n",
    "# setup bins \n",
    "bins = [1945, 1964, 1980, 1996, 2012]\n",
    "\n",
    "# use the `cut()` function to divide years into their appropriate bins\n",
    "cats = pd.cut(year, bins)\n",
    "\n",
    "# returns a special `Categorical` object\n",
    "cats\n",
    "\n",
    "Uses mathematical notation for intervals.\n",
    "- `(` is *open* (not included in the category)\n",
    "- `]` is *closed* (inclusive)\n",
    "- can change which side is closed by passing `right=False`\n",
    "\n",
    "# can pass your own bin names\n",
    "cats = pd.cut(year, bins, labels=['Baby Boomer', 'Generation X', \n",
    "                                  'Generation Y (Millennials)', 'Generation Z'])\n",
    "\n",
    "# label for the year data\n",
    "cats.codes\n",
    "\n",
    "# categories for the year data\n",
    "cats.categories\n",
    "\n",
    "# value counts for the bins\n",
    "pd.value_counts(cats)\n",
    "\n",
    "# create random array of data\n",
    "data = np.random.rand(100)\n",
    "data\n",
    "\n",
    "# equal length bins based on min and max values\n",
    "pd.cut(data, 4, precision=2)\n",
    "\n",
    "# create sample Series data\n",
    "ser1 = pd.Series(np.random.rand(20))\n",
    "ser1[[1,3,5]] = np.nan\n",
    "ser1\n",
    "\n",
    "# bin in four equal categories\n",
    "ser1 = pd.cut(ser1, 4, precision=4, labels=['small','medium','large','x-large'])\n",
    "ser1\n",
    "\n",
    "# notice syntax for Series\n",
    "ser1.cat.codes\n",
    "\n",
    "ser1.cat.categories\n",
    "\n",
    "# add category - added to the front position\n",
    "ser1 = ser1.cat.add_categories('not_measured')\n",
    "ser1\n",
    "\n",
    "# reorder categories so that new category is first\n",
    "ser1 = ser1.cat.reorder_categories(['not_measured','small','medium','large','x-large'])\n",
    "ser1\n",
    "\n",
    "# make categories unordered\n",
    "ser1 = ser1.cat.as_unordered()\n",
    "ser1\n",
    "\n",
    "# fillna with 'not_measured'\n",
    "ser1 = ser1.fillna('not_measured')\n",
    "#ser1 = ser1.fillna('missing')\n",
    "ser1\n",
    "\n",
    "# creating a categorical datatype with automatic categories\n",
    "ser2 = pd.Series(['a','b','a','c','b','a']).astype('category')\n",
    "ser2\n",
    "\n",
    "#### Student Practice\n",
    "Try to perform the following tasks on the `vienna` dataset. Then check your answers as I walk through the solutions. Unless the exercise asks you to create a new DataFrame, you can assume that you are to alter the original `vienna` data.\n",
    "\n",
    "**Exercise:** View the `host_response_time`, `host_response_rate`, and `host_acceptance_rate` columns of the `vienna` dataset.\n",
    "\n",
    "### ENTER CODE HERE ###\n",
    "\n",
    "**Exercise:** For `host_response_time`, code all missing values as `unknown`\n",
    "\n",
    "### ENTER CODE HERE ###\n",
    "\n",
    "**Exercise:** Make the `host_response_time` a categorical datatype.\n",
    "\n",
    "### ENTER CODE HERE ###\n",
    "\n",
    "**Exercise:** First, run the code below to turn the `host_acceptance_rate` and `host_response_rate` from strings to floats. \n",
    "\n",
    "Note that the `regex=True` determines if the passed-in pattern is a regular expression:\n",
    "\n",
    "- If True, assumes the passed-in pattern is a regular expression.\n",
    "\n",
    "- If False, treats the pattern as a literal string\n",
    "\n",
    "# change string values to floats\n",
    "vienna.loc[:,'host_acceptance_rate'] = vienna.loc[:,'host_acceptance_rate'].str.replace('\\%', '', regex=True).astype(float)\n",
    "vienna.loc[:,'host_response_rate'] = vienna.loc[:,'host_response_rate'].str.replace('\\%', '', regex=True).astype(float)\n",
    "\n",
    "**Exercise:** Create the following bins for the `host_acceptance_rate` and `host_response_rate` columns\n",
    "- 0 - 49\n",
    "- 50 - 79\n",
    "- 80 - 89\n",
    "- 90 - 99\n",
    "- 100\n",
    "\n",
    "### ENTER CODE HERE ###\n",
    "\n",
    "**Exercise:** \n",
    "1. Add a new `unknown` category for both `host_response_rate` and `host_acceptance_rate`. \n",
    "2. Re-order the categories so that `unknown` is first.\n",
    "3. Fill all missing values in these two columns with the `unknown` category.\n",
    "\n",
    "### ENTER CODE HERE ###\n",
    "\n",
    "### Detecting & Filtering Outliers<a name=\"outlier\"></a>\n",
    "\n",
    "# create DataFrame of normally distributed data\n",
    "data = pd.DataFrame(np.random.randn(1000, 4))\n",
    "data\n",
    "\n",
    "data.describe()\n",
    "\n",
    "# select values in column 1 that have values exceeding 3 in absolute value\n",
    "column1 = data[1]\n",
    "column1[np.abs(column1) > 3]\n",
    "\n",
    "# select all rows having a value exceeding 3 or -3\n",
    "\n",
    "### old way to code ###\n",
    "# data[(np.abs(data) > 3).any(1)]\n",
    "\n",
    "### new way due to Pandas update ###\n",
    "data[(np.abs(data) > 3).any(axis=1)]\n",
    "\n",
    "# now set the values based on this criteria\n",
    "data[np.abs(data) > 3] = np.sign(data) * 3\n",
    "\n",
    "# look at results\n",
    "\n",
    "### old way to code ###\n",
    "# data[(np.abs(data) == 3).any(1)]\n",
    "\n",
    "### new way due to Pandas update ###\n",
    "data[(np.abs(data) == 3).any(axis=1)]\n",
    "\n",
    "# notice min and max\n",
    "data.describe() \n",
    "\n",
    "# np.sign() produces 1 and -1 values based on positive/negative\n",
    "np.sign(data).head()\n",
    "\n",
    "Now, let's look at the `price` column in the `vienna` dataset.\n",
    "\n",
    "# view price\n",
    "vienna['price']\n",
    "\n",
    "# remove dollar signs and commas; change to float\n",
    "vienna['price'] = vienna['price'].str.replace('\\$', '', regex=True).str.replace(',','').astype(float)\n",
    "\n",
    "# verify results\n",
    "vienna['price']\n",
    "\n",
    "# let's plot the price data\n",
    "sns.kdeplot(data=vienna['price'], fill='fill') # updated code due to seaborn update\n",
    "plt.show()\n",
    "\n",
    "# check min price\n",
    "vienna['price'].min()\n",
    "\n",
    "# remove instances with $0 price\n",
    "vienna = vienna.drop(vienna[vienna['price'] == 0].index)\n",
    "\n",
    "vienna['price'].min()\n",
    "\n",
    "# example to explain previous index code \n",
    "vienna[vienna['price'] == 9.0].index\n",
    "\n",
    "# check max price\n",
    "vienna['price'].max()\n",
    "\n",
    "# 99% of data fall below the following price\n",
    "top = int(vienna['price'].quantile(0.99))\n",
    "top\n",
    "\n",
    "# remove top 1% outliers\n",
    "vienna = vienna.drop(vienna[vienna['price'] > top].index)\n",
    "\n",
    "vienna['price'].max()\n",
    "\n",
    "# check new kde plot with outliers removed\n",
    "sns.kdeplot(data=vienna['price'], fill='fill') # updated code due to seaborn update\n",
    "plt.show()\n",
    "\n",
    "### Dummy Variables<a name=\"dummy\"></a>\n",
    "\n",
    "Convert categorical variables into dummy variables as a lot of machine learning algorithms work specifically with numerical data.\n",
    "\n",
    "# create a simple series\n",
    "ser = pd.Series(['dog','cat','dog','horse'])\n",
    "ser\n",
    "\n",
    "pd.get_dummies(ser)\n",
    "\n",
    "pd.get_dummies(ser, prefix='animal')\n",
    "\n",
    "# reduces correlation and dimensionality of data\n",
    "pd.get_dummies(ser, drop_first=True)\n",
    "\n",
    "ser1 = ['cat','dog',np.nan]\n",
    "ser1\n",
    "\n",
    "pd.get_dummies(ser1)\n",
    "\n",
    "pd.get_dummies(ser1, dummy_na=True)\n",
    "\n",
    "#### Student Practice\n",
    "Try to perform the following tasks on the `vienna` dataset. Then check your answers as I walk through the solutions. Unless the exercise asks you to create a new DataFrame, you can assume that you are to alter the original `vienna` data.\n",
    "\n",
    "**Exercise:** What are the values and counts for the following three columns: `host_is_superhost`, `room_type`, and `property_type`?\n",
    "\n",
    "### ENTER CODE HERE ###\n",
    "\n",
    "**Exercise:** Use `get_dummies()` with the `room_type` and `property_type`columns. Add a prefix of your choosing and drop the first dummy variable for each. *Note: I didn't show you how to work with columns within a DataFrame. See if you can look at the [pandas documentation](https://pandas.pydata.org/docs/reference/api/pandas.get_dummies.html) to figure this out so that you update the `vienna` DataFrame.*\n",
    "\n",
    "### ENTER CODE HERE ###\n",
    "\n",
    "**Exercise:** To make sure you understand what `get_dummies()` is doing, can you take the `host_is_superhost` column and turn the values into dummy variables without using `get_dummies()`? Make sure that you only have one column called `superhost` after you are finished and the original column is deleted.\n",
    "\n",
    "### ENTER CODE HERE ###\n",
    "\n",
    "## String Manipulation<a name=\"string\"></a>\n",
    "\n",
    "### String Object Methods<a name=\"string_object\"></a>\n",
    "\n",
    "One of Pythons most popular strengths is its ability to work with strings and text. Some of the more simple operations can easily be done with the built in string methods.\n",
    "\n",
    "# create simple string of text\n",
    "a = 'Jimmy,Anderson,  jimmy@gmail.com'\n",
    "a\n",
    "\n",
    "# using the split() method\n",
    "a.split(',') # splits text on the comma\n",
    "\n",
    "# strip() removes whitespace from around character\n",
    "b = [x.strip() for x in a.split(',')]\n",
    "b\n",
    "\n",
    "# joining strings within a list\n",
    "'++'.join(b)\n",
    "\n",
    "# count of number of commas\n",
    "a.count(',')\n",
    "\n",
    "# replace strings in text\n",
    "a.replace(',','')\n",
    "\n",
    "There are many more [string methods](https://www.w3schools.com/python/python_ref_string.asp) that you can research, and you will learn more about some of these in the 575 Python class if you haven't already taken it.\n",
    "\n",
    "Now, let's try to clean up the 'messy' `amenities` column from the `vienna` data. \n",
    "\n",
    "# view the amenities for one property\n",
    "vienna.iloc[0]['amenities']\n",
    "\n",
    "# create an empty list\n",
    "all_amenities = []\n",
    "\n",
    "# iterate through rows, extending amenities to list\n",
    "for row in vienna.amenities:\n",
    "    all_amenities.extend(row.replace('[','').replace(']','').replace('\"','').lower().strip().split(','))\n",
    "\n",
    "all_amenities\n",
    "\n",
    "amenities_list = pd.unique(all_amenities)\n",
    "amenities_list\n",
    "\n",
    "### Note: You are not responsible for knowing how to do this part of the code. This is to show you what can be done. ###\n",
    "amenity_values = vienna['amenities']\n",
    "amenity_values\n",
    "\n",
    "# check the following for more information: https://docs.python.org/3/library/collections.html#collections.Counter\n",
    "from collections import Counter\n",
    "\n",
    "# create Series of amenities column\n",
    "amenity_values = vienna['amenities']\n",
    "\n",
    "# instantiate counter object\n",
    "counter = Counter()\n",
    "\n",
    "# update counter with each amenity and its total\n",
    "for response in amenity_values:\n",
    "    counter.update(response.replace('[','').replace(']','').replace('\"','').lower().split(','))\n",
    "\n",
    "# create two empty lists    \n",
    "amenities_list = []\n",
    "amenities_count = []\n",
    "\n",
    "# iterate through most common amenities in counter and append to lists\n",
    "for item in counter.most_common(50):\n",
    "    amenities_list.append(item[0])\n",
    "    amenities_count.append(item[1])\n",
    "    \n",
    "most_common_amenities = zip(amenities_list,amenities_count)\n",
    "print(list(most_common_amenities))\n",
    "\n",
    "def amenities_to_columns(data):\n",
    "    '''\n",
    "    Creates new amenity features if the list of amenities for a respective property includes\n",
    "    certain key words.\n",
    "    \n",
    "    Returns: DataFrame\n",
    "    '''\n",
    "    # create list of new amenity columns\n",
    "    new_columns = ['tv','longterm','washing','cooking','fridge','fireplace','free_parking','paid_parking',\n",
    "                   'air_conditioning','balcony','water_access','coffee','breakfast','family','workspace']\n",
    "    \n",
    "    #search 'amenities' feature for keywords, create new column and dummy variable if true\n",
    "    data.loc[data['amenities'].str.contains('tv|cable|amazon prime|netflix', case=False), 'tv'] = 1\n",
    "    data.loc[data['amenities'].str.contains('long term', case=False), 'longterm'] = 1\n",
    "    data.loc[data['amenities'].str.contains('washer|dishwasher', case=False), 'washing'] = 1\n",
    "    data.loc[data['amenities'].str.contains('stove|oven|microwave', case=False), 'cooking'] = 1\n",
    "    data.loc[data['amenities'].str.contains('refrigerator|freezer|fridge', case=False), 'fridge'] = 1\n",
    "    data.loc[data['amenities'].str.contains('fireplace|fire pit', case=False), 'fireplace'] = 1\n",
    "    data.loc[data['amenities'].str.contains('free parking|free street parking', case=False), 'free_parking'] = 1\n",
    "    data.loc[data['amenities'].str.contains('paid parking|paid street parking',case=False), 'paid_parking'] = 1\n",
    "    data.loc[data['amenities'].str.contains('air conditioning|central air conditioning', case=False), 'air_conditioning'] = 1\n",
    "    data.loc[data['amenities'].str.contains('balcony|patio', case=False), 'balcony'] = 1\n",
    "    data.loc[data['amenities'].str.contains('hot tub|waterfront|pool|lake|beachfront', case=False), 'water_access'] = 1\n",
    "    data.loc[data['amenities'].str.contains('coffee|coffee machine|nespresso', case=False), 'coffee'] = 1\n",
    "    data.loc[data['amenities'].str.contains('breakfast', case=False), 'breakfast'] = 1\n",
    "    data.loc[data['amenities'].str.contains('high chair|crib|children|child|baby|family', case=False), 'family'] = 1\n",
    "    data.loc[data['amenities'].str.contains('workspace', case=False), 'workspace'] = 1\n",
    "\n",
    "    # replace na's with 0\n",
    "    data[new_columns] = data[new_columns].fillna(0)\n",
    "    \n",
    "    return data\n",
    "    \n",
    "vienna = amenities_to_columns(vienna)\n",
    "\n",
    "vienna = vienna.drop('amenities', axis=1)\n",
    "vienna.head()\n",
    "\n",
    "### Regular Expressions<a name=\"regex\"></a>\n",
    "\n",
    "While string methods are powerful and can be used in a lot of different ways, sometimes you need a way to search a more complex string pattern. This is where regular expressions (`regex`) offer a flexible way to search or match complex patterns.\n",
    "\n",
    "Regex could be an entire module on its own, but we will cover some of the basics now.\n",
    "\n",
    "https://www.dataquest.io/wp-content/uploads/2019/03/python-regular-expressions-cheat-sheet.pdf\n",
    "\n",
    "# import the re module\n",
    "import re\n",
    "\n",
    "# create a simple text message\n",
    "text = 'A     Lannister\\tnever\\nforgets'\n",
    "\n",
    "# split strings based on whitespace characters (tabs, spaces, newlines)\n",
    "re.split('\\s+', text)\n",
    "\n",
    "When you call `re.split()` it is first compiled, then the split method is called on the passed text. You can compile the regex yourself, which is highly recommended if you apply the same expression to many strings. This will also save you CPU cycles. \n",
    "\n",
    "# setup text of names and emails\n",
    "emails = \"\"\"\n",
    "James james.anderson@eastern.edu\n",
    "Sally sally@gmail.com\n",
    "Ryan ryan22@yahoo.com\n",
    "\"\"\"\n",
    "\n",
    "# creating pattern using raw string literal\n",
    "pattern = r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b' \n",
    "\n",
    "# compile the regex\n",
    "regex = re.compile(pattern, flags=re.IGNORECASE)\n",
    "\n",
    "# returns all matches in a string\n",
    "regex.findall(emails)\n",
    "\n",
    "# returns only the first match\n",
    "position = regex.search(emails) # first email, start and end position\n",
    "position\n",
    "\n",
    "emails[position.start():position.end()]\n",
    "\n",
    "# only matches the beginning of the string\n",
    "print(regex.match(emails)) #only match if pattern occurs at the start of the string\n",
    "\n",
    "# simple program to validate an email\n",
    "\n",
    "#create email list\n",
    "email_list = ['james.anderson@eastern.edu','sally@gmail.com','ryan22@yahoo']\n",
    "\n",
    "# iterate through emails\n",
    "for email in email_list:\n",
    "    if (re.fullmatch(regex, email)):\n",
    "        print(\"Valid Email\")\n",
    " \n",
    "    else:\n",
    "        print(\"Invalid Email\")\n",
    "        \n",
    "\n",
    "# returns new string with occurrences of the pattern replaced by the new string\n",
    "print(regex.sub('REDACTED', emails))\n",
    "\n",
    "# wrap parentheses around different segments\n",
    "pattern2 = r'(\\b[A-Za-z0-9._%+-]+)@([A-Za-z0-9.-]+)\\.([A-Z|a-z]{2,}\\b)'\n",
    "\n",
    "# compile with new pattern\n",
    "regex = re.compile(pattern2, flags=re.IGNORECASE)\n",
    "\n",
    "# returns tuple of matched components\n",
    "segments = regex.match('jamie.andrews@eastern.edu')\n",
    "segments\n",
    "\n",
    "segments.groups()\n",
    "\n",
    "Let's look at a practical example using our `vienna` dataset. We need to turn the strings as listed in the `bathrooms_text` column to floats.\n",
    "\n",
    "# view column\n",
    "vienna['bathrooms_text'].value_counts()\n",
    "\n",
    "pattern = r\"[-+]?\\d*\\.\\d+|\\d+\"\n",
    "# [-+] --> handles plus or minus signs\n",
    "# ? -->  matches the expression to its left 0 or 1 times.\n",
    "# \\d --> matches digits\n",
    "# * --> matches the expression to its left 0 or more times.\n",
    "# \\. --> escapes the decimal character\n",
    "# + --> y matches the expression to its left 1 or more times.\n",
    "# | --> matches expression before or after \n",
    "\n",
    "\n",
    "def bathroomtext_to_num(text):\n",
    "    '''\n",
    "    Strips out number from bathrooms_text field \n",
    "    \n",
    "    Returns: float\n",
    "    '''\n",
    "    pattern = r\"[-+]?\\d*\\.\\d+|\\d+\"  \n",
    "    regex = re.compile(pattern)\n",
    "    num_baths = regex.findall(str(text))\n",
    "    \n",
    "    if 'half' in str(text).lower():\n",
    "        return 0.5\n",
    "    elif num_baths == []:\n",
    "        return 1\n",
    "    else:\n",
    "        return num_baths[0]\n",
    "\n",
    "# apply custom function to 'bathrooms_text' column\n",
    "vienna['bathrooms'] = vienna.apply(lambda row: bathroomtext_to_num(row['bathrooms_text']), axis=1).astype('float64')\n",
    "\n",
    "\n",
    "# view new 'bathrooms' feature\n",
    "vienna[['bathrooms','bathrooms_text']].head(10)\n",
    "\n",
    "vienna = vienna.drop('bathrooms_text', axis=1)\n",
    "\n",
    "vienna.head()  \n",
    "\n",
    "## Extra Practice: Plotting and Miscellaneous<a name='extra'></a>\n",
    "\n",
    "**Exercise:** Plot a [scatterplot](https://seaborn.pydata.org/generated/seaborn.scatterplot.html) of longitude and latitude using the 'free_parking' attribute as the color. What do you notice about properties with free parking?\n",
    "\n",
    "### ENTER CODE HERE ###\n",
    "\n",
    "**Exercise:** Plot a scatterplot of longitude and latitude using price as the color. Do you notice any discerable pattern in terms of price versus the area of Vienna?\n",
    "\n",
    "### ENTER CODE HERE ###\n",
    "\n",
    "**Exercise:** Plot a [violin plot](https://seaborn.pydata.org/generated/seaborn.violinplot.html) using the price and the neighborhood name. What neighborhoods seem to have the largest range and highest median prices?\n",
    "\n",
    "### ENTER CODE HERE ###\n",
    "\n",
    "**Exercise:** What areas have the most listings? Create a [horizontal bar chart](https://seaborn.pydata.org/generated/seaborn.countplot.html) to show these listings by neighborhood.\n",
    "\n",
    "### ENTER CODE HERE ###\n",
    "\n",
    "**Exercise:** Using a pivot table, show the top hosts (represented by `host_id`) that have the most multiple listings (`calculated_host_listings_count`).\n",
    "\n",
    "### ENTER CODE HERE ###\n",
    "\n",
    "**Exercise:** How many beds, bedrooms, and bathrooms do most properties have? What percentage of properties have those respective most common values?\n",
    "\n",
    "### ENTER CODE HERE ###\n",
    "\n",
    "**Exercise:** Similar to how we binned the `host_response_time`, `host_response_rate`, and `host_acceptance_rate`, perform a similar binning for the various review scores attributes.\n",
    "    \n",
    "- `review_scores_rating`: bin the review scores between '0-79', '80-94','95-99' and '100'.\n",
    "- other review ratings: bin these scores between '0-8','9', and '10'\n",
    "- add an unknown category and fill the na's with 'unkown' (don't forget to reorder your categories so that 'unknown' is first)\n",
    "\n",
    "### ENTER CODE HERE ###\n",
    "\n",
    "## Conclusion<a name=\"conclusion\"></a>\n",
    "\n",
    "In reality, a lot more needs to be done to this dataset before it is ready for a machine learning algorithm to be run. Also, as I mentioned above, there are sometimes easier ways to perform some of these data manipulations using Scikit-learn, which you will learn about more in the machine learning classes.\n",
    "\n",
    "After cleaning up the data further, checking for correlation between the attributes, dropping attributes and creating some new ones using this data, the best machine learning model was able to predict **66%** of the price using this data with a RMSE score of **0.3643** (the target value was changed to use the log of the price).\n",
    "\n",
    "Given the above results, my conclusion is that this data can be used to predict prices somewhat but more work or more data is needed to create a better model. Future work could involve:\n",
    "\n",
    "- performing analysis on the photos and determine if they have an impact on price\n",
    "- performing a sentiment analysis on the foreign language comments (from a different file on the InsideAirbnb website) to determine if the guest comments could be an indicator of price. I was able to perform a sentiment analysis on the English language comments and there was not much impact on the model.\n",
    "- further feature extraction/elimination\n",
    "- prices are the listed prices from the Airbnb website. A more accurate model might be obtained by using the prices that guests actually paid for a property\n",
    "- personally scrape data from the Aribnb website to obtain more information than what is listed in the file"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
